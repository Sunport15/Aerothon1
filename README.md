<h1 align="center" > Hi, This is CodeFellas ğŸ‘‹</h1>
</hr>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/devicons/devicon@v2.15.1/devicon.min.css">
          
<h3> ğŸ‘¨ğŸ»â€ğŸ’» &nbsp;Problem Statement </h3>

Problem Statement
A washing machine manufacturing company makes a lot of washing machines every year.
There are different models and types. And for each type of washing machine, the technology
and logistics are different and these items are being manufactured in different parts of the world.
All of this is aligned in a supply chain management process with the targeted dates of delivery
planned for next 5 years.
As this is a complex process, each department produces a lot of data related to the logistics,
supply chain, planning, execution and forecasting of orders and other details. This often means,
not only the department that owns a certain type of data produces it, but also the other
departments who are either the direct or indirect consumers of the data also produce a possible
forecast data. These departments produce the forecast data to keep up with the planning and
their day to day activities, instead of waiting for it when it finally reaches them. At the same time,
the data owning departments also keep updating the data based on everyday changes, which is
an overhead for the consuming teams who have already consumed it and now need to
recalibrate their data.
Also, all this data is finally consolidated when the official manufacturing process has achieved
them(real-time data).
The intermediate process ends up creating a lot of data by each department, which are then
consumed by other departments or sub manufacturing units to plan their logistics and supply
Chain.
This intermediate data which gets generated based on milestones
achievement in the production process, is mostly a redundant data without any authenticity.
This lies in the system consuming a lot of space and memory and in long term and creates
sustainability issues.
In the dataset provided, 3 stages in the manufacturing are put-up, fabrication, sub-assembly and
assembly. (Dataset provided is just an example. Use it to extend the dataset)
Provide a possible solution and approach to reduce this underlying intermediate data in the
system and make the approach of departments using the data more sustainable in the long
term.

<h3> ğŸ‘¨ğŸ»â€ğŸ’» &nbsp;Requirements</h3>
â— Create a data lake with a normalized DB to reduce the redundancy.
</hr>
â— Identify the current redundant data from the forecasted data.
</hr>
â— Create an automation process for data stamping(approval) the real time data.
</hr>
â— Create a dashboard for the users in each domain to access the data required for their domain and also allow the forecast and real time data creation.
</hr>
â— Create a dashboard for the data officer to monitor the data stamping process.
</hr>
<a href="https://github.com/Sunport15">
<!--   <img height="180em" src="https://github-readme-stats.vercel.app/api?username=Sunport15&theme=buefy&show_icons=true" /> -->
  <img height="180em" src="https://github-readme-stats.vercel.app/api/top-langs/?username=Sunport15&theme=buefy&layout=compact" />
</a>
</hr>
<h2>My Tech Stack & Tools</h2>
<img align="left"src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original-wordmark.svg" / style="height: 30px; width: 30px;">
<img align="left"src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original-wordmark.svg" / style="height: 30px; width: 30px;">
<img align="left"src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pandas/pandas-original-wordmark.svg" / style="height: 30px; width: 30px;">
<img align="left"src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/vscode/vscode-original.svg" / style="height: 30px; width: 30px;">
</hr>
<h3> ğŸ‘¨ğŸ»â€ğŸ’» &nbsp;Future Aspects </h3>
â— Cloud computing can help to increase the speed and scalability of data processing. Tools like Amazon Web Services (AWS) and Microsoft Azure can be used to run machine learning algorithms and store large datasets in the cloud.
</hr>
â— Distributed computing frameworks like Apache Hadoop and Apache Spark can be used to speed up data processing and analysis by distributing tasks across multiple computers.
